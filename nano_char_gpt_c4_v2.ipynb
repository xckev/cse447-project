{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBcH16sHDCRn"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers datasets huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjlbA60ADx5f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, interleave_datasets\n",
        "from huggingface_hub import login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "login()"
      ],
      "metadata": {
        "id": "lFOVNT7GWRoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkHR4wFBEvUS"
      },
      "outputs": [],
      "source": [
        "!gcloud auth login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKHBDUCLD0Z4"
      },
      "outputs": [],
      "source": [
        "#ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "# Streaming mC4 with English bias (50% English, 1 epoch target)\n",
        "TOTAL_ROWS = 2_000_000       # Updated to 1m\n",
        "ENGLISH_ROWS = 1_000_000     # 50%\n",
        "OTHER_ROWS = 10_000         # ~1m / 100 languages\n",
        "\n",
        "ENGLISH_VAL_ROWS = 10_000  # Scaled down validation\n",
        "OTHER_VAL_ROWS = 100\n",
        "\n",
        "LANGUAGES = [\n",
        "    \"af\", \"am\", \"ar\", \"az\", \"be\", \"bg\", \"bn\", \"ca\", \"ceb\", \"co\", \"cs\", \"cy\", \"da\", \"de\",\n",
        "    \"el\", \"eo\", \"es\", \"et\", \"eu\", \"fa\", \"fi\", \"fil\", \"fr\", \"fy\", \"ga\", \"gd\", \"gl\", \"gu\",\n",
        "    \"ha\", \"haw\", \"iw\", \"hi\", \"hmn\", \"ht\", \"hu\", \"hy\", \"id\", \"ig\", \"is\", \"it\", \"ja\",\n",
        "    \"jv\", \"ka\", \"kk\", \"km\", \"kn\", \"ko\", \"ku\", \"ky\", \"la\", \"lb\", \"lo\", \"lt\", \"lv\", \"mg\",\n",
        "    \"mi\", \"mk\", \"ml\", \"mn\", \"mr\", \"ms\", \"mt\", \"my\", \"ne\", \"nl\", \"no\", \"ny\", \"pa\", \"pl\",\n",
        "    \"ps\", \"pt\", \"ro\", \"ru\", \"sd\", \"si\", \"sk\", \"sl\", \"sm\", \"sn\", \"so\", \"sq\", \"sr\", \"st\",\n",
        "    \"su\", \"sv\", \"sw\", \"ta\", \"te\", \"tg\", \"th\", \"tr\", \"uk\", \"und\", \"ur\", \"uz\", \"vi\", \"xh\",\n",
        "    \"yi\", \"yo\", \"zh\", \"zu\"\n",
        " ]\n",
        "\n",
        "en_train = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True).select_columns([\"text\"])\n",
        "en_val = load_dataset(\"allenai/c4\", \"en\", split=\"validation\", streaming=True).select_columns([\"text\"])\n",
        "\n",
        "\n",
        "other_train_list = [\n",
        "    load_dataset(\"allenai/c4\", lang, split=\"train\", streaming=True).take(OTHER_ROWS).select_columns([\"text\"])\n",
        "    for lang in LANGUAGES\n",
        "]\n",
        "other_val_list = [\n",
        "    load_dataset(\"allenai/c4\", lang, split=\"validation\", streaming=True).take(OTHER_VAL_ROWS).select_columns([\"text\"])\n",
        "    for lang in LANGUAGES\n",
        "]\n",
        "\n",
        "train_parts = [en_train.take(ENGLISH_ROWS)] + other_train_list\n",
        "val_parts = [en_val.take(ENGLISH_VAL_ROWS)] + other_val_list\n",
        "\n",
        "train_ds = interleave_datasets(train_parts, seed=42, stopping_strategy=\"all_exhausted\")\n",
        "val_ds = interleave_datasets(val_parts, seed=42, stopping_strategy=\"all_exhausted\")\n",
        "\n",
        "SPECIAL_TOKENS_LIST = [\"<pad>\", \"<s>\", \"</s>\", \"<unk>\"]\n",
        "SPECIAL_TOKENS = {tok: i for i, tok in enumerate(SPECIAL_TOKENS_LIST)}\n",
        "offset = len(SPECIAL_TOKENS_LIST)\n",
        "\n",
        "pad_token_id = SPECIAL_TOKENS[\"<pad>\"]\n",
        "bos_token_id = SPECIAL_TOKENS[\"<s>\"]\n",
        "eos_token_id = SPECIAL_TOKENS[\"</s>\"]\n",
        "unk_token_id = SPECIAL_TOKENS[\"<unk>\"]\n",
        "\n",
        "MAX_SEQ_LEN = 256\n",
        "\n",
        "# Pure-Python character split (mirrors AllenNLP when byte_encoding=None).\n",
        "def chars_from_text(text):\n",
        "    return list(text)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01414339"
      },
      "source": [
        "print(\"Checking the first 10 samples of the interleaved stream:\\n\")\n",
        "# We take 10 samples to see the mixing pattern\n",
        "for i, example in enumerate(train_ds.take(30)):\n",
        "    # Print the first 80 chars to identify the language\n",
        "    text_sample = example['text'][:80].replace('\\n', ' ')\n",
        "    print(f\"Sample {i+1}: {text_sample}...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBNIlmdg30zN"
      },
      "outputs": [],
      "source": [
        "def build_char_vocab(dataset):\n",
        "    char_set = set()\n",
        "    for example in dataset:\n",
        "        for ch in chars_from_text(example[\"text\"]):\n",
        "            char_set.add(ch)\n",
        "    id2char = sorted(char_set)\n",
        "    char2id = {ch: i + offset for i, ch in enumerate(id2char)}\n",
        "    return char2id, id2char\n",
        "\n",
        "print(\"Building character vocab...\")\n",
        "# Use a subset to build vocab quickly (scanning 4.5M rows would take hours)\n",
        "# 400,000 samples should be enough to cover characters from all 100+ languages\n",
        "char2id, id2char = build_char_vocab(train_ds.take(400000))\n",
        "vocab_size = offset + len(id2char)\n",
        "print(f\"Vocab size: {vocab_size} (chars: {len(id2char)})\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "# Free up GPU memory from previous runs to prevent OOM\n",
        "if 'model' in globals():\n",
        "    del model\n",
        "if 'trainer' in globals():\n",
        "    del trainer\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "config = GPT2Config(\n",
        "    vocab_size=vocab_size,\n",
        "    n_positions=MAX_SEQ_LEN,\n",
        "    n_embd=768,\n",
        "    n_layer=12,\n",
        "    n_head=12,\n",
        "    bos_token_id=bos_token_id,\n",
        "    eos_token_id=eos_token_id,\n",
        "    pad_token_id=pad_token_id,\n",
        " )\n",
        "\n",
        "model = GPT2LMHeadModel(config).to(\"cuda\")\n",
        "print(f\"Total Parameters: {model.num_parameters():,}\")"
      ],
      "metadata": {
        "id": "fo1AofIWh57U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyxuFdl4OiIt"
      },
      "outputs": [],
      "source": [
        "def chars_tokenize(examples):\n",
        "    all_encoded = []\n",
        "    for text in examples[\"text\"]:\n",
        "        tokens = chars_from_text(text)\n",
        "        encoded = [char2id.get(ch, unk_token_id) for ch in tokens][:MAX_SEQ_LEN]\n",
        "        all_encoded.append(encoded)\n",
        "    return {\"input_ids\": all_encoded}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDBCp3vOD44V"
      },
      "outputs": [],
      "source": [
        "tokenized_ds = train_ds.map(chars_tokenize, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# FIX: Limit validation set to 512 samples so evaluation takes seconds, not hours\n",
        "tokenized_val_ds = val_ds.take(512).map(chars_tokenize, batched=True, remove_columns=[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "vocab_data = {'char2id': char2id, 'id2char': id2char}\n",
        "with open('vocab.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(vocab_data, f, ensure_ascii=False)\n",
        "# Then download and copy to work/vocab.json\n",
        "import json\n",
        "\n",
        "vocab_data = {'char2id': char2id, 'id2char': id2char}\n",
        "with open('vocab.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(vocab_data, f, ensure_ascii=False)\n",
        "\n",
        "BUCKET_NAME = \"cse447\"\n",
        "print(f\"Uploading vocab to gs://{BUCKET_NAME}/nano-char-gpt-c4-v2/vocab.json...\")\n",
        "!gcloud storage cp ./vocab.json gs://{BUCKET_NAME}/nano-char-gpt-c4-v2/vocab.json\n",
        "print(\"Vocab upload complete!\")"
      ],
      "metadata": {
        "id": "jE_hf7iRIDty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y85ybx1HD8r_"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "MAX_STEPS = TOTAL_ROWS // BATCH_SIZE\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./nano-char-gpt-c4-v2\",\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    max_steps=MAX_STEPS,   # Auto-calculated\n",
        "    learning_rate=1e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.1,\n",
        "\n",
        "    # L4 Specific Speed Boosts\n",
        "    bf16=True,             # Faster/more stable than fp16 on L4\n",
        "    tf32=True,             # Speed up internal math\n",
        "    dataloader_num_workers=1, # Set to 1 to avoid warning with streaming datasets\n",
        "\n",
        "    save_total_limit=1,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"steps\", # Updated from evaluation_strategy\n",
        "    eval_steps=100,\n",
        "    report_to=\"none\"\n",
        "  )\n",
        "\n",
        "\n",
        "def manual_collator(features):\n",
        "    # Find the longest sequence in the batch\n",
        "    max_len = max(len(f[\"input_ids\"]) for f in features)\n",
        "\n",
        "    batch_input_ids = []\n",
        "    batch_attention_mask = []\n",
        "    batch_labels = []\n",
        "\n",
        "    for f in features:\n",
        "        pad_len = max_len - len(f[\"input_ids\"])\n",
        "        padded_ids = f[\"input_ids\"] + [pad_token_id] * pad_len\n",
        "        padded_mask = [1] * len(f[\"input_ids\"]) + [0] * pad_len\n",
        "        labels = f[\"input_ids\"] + [-100] * pad_len\n",
        "\n",
        "        batch_input_ids.append(padded_ids)\n",
        "        batch_attention_mask.append(padded_mask)\n",
        "        batch_labels.append(labels)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": torch.tensor(batch_input_ids, dtype=torch.long),\n",
        "        \"attention_mask\": torch.tensor(batch_attention_mask, dtype=torch.long),\n",
        "        \"labels\": torch.tensor(batch_labels, dtype=torch.long)\n",
        "    }\n",
        "\n",
        "# Update your trainer to use this collator\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_ds,\n",
        "    eval_dataset=tokenized_val_ds,\n",
        "    data_collator=manual_collator # Use our custom function\n",
        "  )\n",
        "\n",
        "model = torch.compile(model)\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"./nano-char-gpt-c4-v2\")\n",
        "\n",
        "BUCKET_NAME = \"cse447\"\n",
        "\n",
        "print(f\"Uploading model to gs://{BUCKET_NAME}...\")\n",
        "!gcloud storage cp -r ./nano-char-gpt-c4-v2 gs://{BUCKET_NAME}/nano-char-gpt-c4-v2/\n",
        "print(\"Upload Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Retry loop to handle streaming/network connection drops\n",
        "MAX_RETRIES = 100\n",
        "retry_count = 0\n",
        "\n",
        "while retry_count < MAX_RETRIES:\n",
        "    try:\n",
        "        print(f\"\\n=== Training Attempt {retry_count + 1} ===\")\n",
        "        # resume_from_checkpoint=True automatically picks the latest checkpoint in output_dir\n",
        "        trainer.train(resume_from_checkpoint=True)\n",
        "        print(\"Training completed successfully!\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTraining interrupted with error: {e}\")\n",
        "        print(\"Wait 30 seconds before retrying to clear connection issues...\")\n",
        "        time.sleep(30)\n",
        "        retry_count += 1\n",
        "\n",
        "if retry_count == MAX_RETRIES:\n",
        "    print(\"Max retries reached. Training failed.\")\n",
        "else:\n",
        "    # Only save and upload if we finished successfully\n",
        "    trainer.save_model(\"./nano-char-gpt-c4-v2\")\n",
        "\n",
        "    BUCKET_NAME = \"cse447\"\n",
        "\n",
        "    print(f\"Uploading model to gs://{BUCKET_NAME}...\")\n",
        "    !gcloud storage cp -r ./nano-char-gpt-c4-v2 gs://{BUCKET_NAME}/nano-char-gpt-c4-v2/\n",
        "    print(\"Upload Complete!\")"
      ],
      "metadata": {
        "id": "hgsKCp8j_o_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boKcsobZHvB2"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"cse447\"\n",
        "print(\"Downloading model from GCS...\")\n",
        "!mkdir -p ./nano-char-gpt-c4-v2-downloaded\n",
        "!gcloud storage cp -r gs://{BUCKET_NAME}/nano-char-gpt-c4-v2/nano-char-gpt-c4-v2 ./nano-char-gpt-c4-v2-downloaded\n",
        "print(\"Download complete!\")\n",
        "\n",
        "local_model_path = \"./nano-char-gpt-c4-v2-downloaded/nano-char-gpt-c4-v2\"\n",
        "print(\"Loading model into memory...\")\n",
        "model = GPT2LMHeadModel.from_pretrained(local_model_path).to(\"cuda\")\n",
        "print(\"Model loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnsxmVdvEBh0"
      },
      "outputs": [],
      "source": [
        "def decode_id(tok_id):\n",
        "    if tok_id < offset:\n",
        "        return SPECIAL_TOKENS_LIST[tok_id]\n",
        "    return id2char[tok_id - offset]\n",
        "\n",
        "\n",
        "def encode_text(text):\n",
        "    tokens = chars_from_text(text)\n",
        "    return [char2id.get(ch, unk_token_id) for ch in tokens]\n",
        "\n",
        "\n",
        "def predict_next_token_top3(text):\n",
        "    encoded = encode_text(text)[-MAX_SEQ_LEN:]\n",
        "    if not encoded:\n",
        "        encoded = [bos_token_id]\n",
        "\n",
        "    input_ids = torch.tensor([encoded], dtype=torch.long).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids).logits[:, -1, :] # Last position\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    top_k_probs, top_k_ids = torch.topk(probs, 3)\n",
        "\n",
        "    for i in range(3):\n",
        "        tok_id = top_k_ids[0][i].item()\n",
        "        token_text = decode_id(tok_id)\n",
        "        if token_text == \" \":\n",
        "            token_display = \"Space\"\n",
        "        elif len(token_text) == 1 and ord(token_text) < 32:\n",
        "            token_display = f\"Control ({tok_id})\"\n",
        "        else:\n",
        "            token_display = token_text\n",
        "        print(f\"Rank {i+1}: '{token_display}' - {top_k_probs[0][i]:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDW2GK4kEDFT"
      },
      "outputs": [],
      "source": [
        "predict_next_token_top3(\"one small step for mankind\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}